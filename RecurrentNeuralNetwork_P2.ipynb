{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from rnn2_utils import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "print('num of classes:', n_categories)\n",
    "print('num of letters:', n_letters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 8.3 Defining the network\n",
    "\n",
    "\n",
    "Define a single directional RNN model.\n",
    "\n",
    "**Instructions**\n",
    "- An RNN cell `rnn_cell`, a fully-connected layer `self.fc`, and a softmax layer `self.softmax` are defined in `__init__()`\n",
    "- The computational graph is defined in `forward()`. The hidden state of the RNN cell is stored in the tensor `h`, which is of size `[batch_size, hidden_size]`. `forward()` takes as input the tensor `x` in of size `[seq_length, batch_size, input_size]`, whose first dimension is the time steps. Therefore, we use a `for` loop to iterate over the time steps, and for each time step, we feed the input at the current step, and the previous hidden state to the RNN cell. The cell then returns a new hidden state, which overrides `h`.\n",
    "- Refer to the original tutorial and the API for `nn.RNNCell` for detailed information: https://pytorch.org/docs/stable/nn.html#rnncell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.rnn_cell = nn.RNNCell(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: size [seq_length, 1, input_size]\n",
    "        \"\"\"\n",
    "        h = torch.zeros(x.size(1), self.hidden_size)\n",
    "        \n",
    "        for i in range(x.size(0)):\n",
    "            ### START YOUR CODE ###\n",
    "            h = self.rnn_cell(x[i], h)\n",
    "            ### END YOUR CODE ###\n",
    "        \n",
    "        ### START YOUR CODE ###\n",
    "        # Hint: first call fc, then call softmax\n",
    "        output = self.fc(h)\n",
    "        out = self.softmax(output)\n",
    "        ### END YOUR CODE ###\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Task 7.3\n",
    "torch.manual_seed(0)\n",
    "rnn = RNN(10, 20, 18)\n",
    "input_data = torch.randn(6, 3, 10)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = rnn(input_data)\n",
    "    \n",
    "print(out.size())\n",
    "print(out[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "\n",
    "|&nbsp;|\n",
    "|--|\n",
    "|torch.Size([3, 18])|\n",
    "|tensor([-3.3146, -3.0216, -2.9363, -3.1215, -2.7141, -2.7103, -2.9526, -3.0657,|\n",
    "       | -2.9442, -3.0038, -2.6818, -2.7880, -2.9849, -3.1164, -2.5659, -2.5336,|\n",
    "       | -2.9586, -2.9764])|\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8.4 Training the model\n",
    "\n",
    "\n",
    "Train the model with stachastic gradient descent. Due to the setting of the origianl tutorrial, one data example is used per iteration, i.e., the mini-batch size is 1.\n",
    "\n",
    "**Instructions**\n",
    "- Each training data example contains a name (`x`) and a label indicating its nationality (`y`). \n",
    "- The model uses the tensor version of `x` and `y`, i.e., `x_tensor` and `y_tensor`. Here `x_tensor` is of size `[seq_length, 1, input_size]`, in which `input_size` is the total number of letters.\n",
    "- `x_tensor[0, 0, :]` as the one-hot encoding for the first letter in the string `x`, and so forth for `x_tensor[1, 0, :]` etc.\n",
    "- The function will output the predicing result every 5000 iterations. You can find a general trend that the prediction is getting more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, n_iters = 100000, print_every = 5000, plot_every = 1000, learning_rate = 0.005):\n",
    "    # Turn on the training model\n",
    "    model.train()\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "    \n",
    "    running_loss = 0\n",
    "    all_losses = []\n",
    "    \n",
    "    # Train loop\n",
    "    start = time.time()\n",
    "    for i in range(n_iters):\n",
    "        y, x, y_tensor, x_tensor = randomTrainingExample()\n",
    "        \n",
    "        ### START YOUR CODE ###\n",
    "        # zero grad\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model.forward(x_tensor)\n",
    "        loss = criterion(output, y_tensor)\n",
    "        \n",
    "        # Backprop and update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ### END YOUR CODE ###\n",
    "        \n",
    "        # Record loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Print iter, loss, name, and guess\n",
    "        if i % print_every == 0 and i > 0:\n",
    "            guess, guess_i = categoryFromOutput(output)\n",
    "            correct = '✓' if guess == y else '✗ (%s)' % y\n",
    "            print('%d %d%% (%s) %.4f %s / %s %s' % (i, i / n_iters * 100, timeSince(start), loss, x, guess, correct))\n",
    "        \n",
    "        # Append loss\n",
    "        if i % plot_every == 0 and i > 0:\n",
    "            all_losses.append(running_loss / plot_every)\n",
    "            running_loss = 0\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure()\n",
    "    plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Task 7.4\n",
    "# Be patient with the training speed :)\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "n_hidden = 128\n",
    "rnn = RNN(n_letters, n_hidden, n_categories)\n",
    "\n",
    "train(rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "\n",
    "|&nbsp;|&nbsp;|\n",
    "|--|--|\n",
    "|5000 5% (0m 8s) | 3.1065 Ba / Vietnamese ✗ (Arabic)|\n",
    "|10000 10% (0m 17s)| 1.9009 Silva / Spanish ✗ (Portuguese)|\n",
    "|15000 15% (0m 27s)| 0.9776 Salucci / Italian ✓|\n",
    "|20000 20% (0m 36s)| 1.5460 Chaim / Korean ✗ (Chinese)|\n",
    "|25000 25% (0m 47s)| 1.9412 Cruz / Spanish ✗ (Portuguese)|\n",
    "|30000 30% (0m 57s)| 2.6189 Neusser / Dutch ✗ (Czech)|\n",
    "|35000 35% (1m 8s) |0.4944 Ribeiro / Portuguese ✓|\n",
    "|40000 40% (1m 18s)| 0.3007 Daher / Arabic ✓|\n",
    "|45000 45% (1m 28s)| 1.5447 Guang / Vietnamese ✗ (Chinese)|\n",
    "|50000 50% (1m 38s)| 0.9920 Mackenzie / Scottish ✓|\n",
    "|55000 55% (1m 48s)| 2.6635 Moon / English ✗ (Korean)|\n",
    "|60000 60% (1m 59s)| 2.5376 Boutros / Portuguese ✗ (Arabic)|\n",
    "|65000 65% (2m 9s) |0.5595 Schuttmann / German ✓|\n",
    "|70000 70% (2m 19s)| 1.6059 Spada / Japanese ✗ (Italian)|\n",
    "|75000 75% (2m 30s)| 1.1130 Coilean / Irish ✓|\n",
    "|80000 80% (2m 40s)| 0.6820 Schneider / German ✓|\n",
    "|85000 85% (2m 50s)| 0.3628 Lopez / Spanish ✓|\n",
    "|90000 90% (3m 1s) |1.1985 Tremblay / French ✓|\n",
    "|95000 95% (3m 12s)| 1.0904 Knopf / German ✓|\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8.5 Bi-directional RNN model\n",
    "\n",
    "\n",
    "Implement a bi-directional RNN model by modifying the architecture of the `RNN` class.\n",
    "\n",
    "**Instructions**\n",
    "- Two distinct RNN cells are defined in `__init__()`. The input size for the fully-connected layer is doubled, because it needs take input the concatenated hidden states from the two cells.\n",
    "- In `forward()`, two hidden states, `h1` and `h2`, are computed separately. `h1` is exactly the same as the previous `h` in one-directional `RNN`. `h2` is computed by reversing the order of the `for` loop: iterating from the last time step to the first time step.\n",
    "- Concatenate `h1` and `h2` using `torch.cat()`. Notice that the correct dimension needs be specified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(BiRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.rnn_cell1 = nn.RNNCell(input_size, hidden_size)\n",
    "        self.rnn_cell2 = nn.RNNCell(input_size, hidden_size)\n",
    "        \n",
    "        self.fc = nn.Linear(2 * hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: size [seq_length, 1, input_size]\n",
    "        \"\"\"\n",
    "        ### START YOUR CODE ###\n",
    "        h1 = torch.zeros(x.size(1), self.hidden_size)\n",
    "        for i in range(x.size(0)):\n",
    "            h1 = self.rnn_cell1(x[i], h1)\n",
    "        \n",
    "        h2 = torch.zeros(x.size(1), self.hidden_size)\n",
    "        for i in reversed(range(x.size(0))): # Hint: reverse the order of the for loop\n",
    "            h2 = self.rnn_cell2(x[i], h2)\n",
    "        \n",
    "        h = torch.cat((h1, h2), 1)\n",
    "        output = self.fc(h)\n",
    "        out = self.softmax(output)\n",
    "        ### END YOUR CODE ###\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Task 8.5\n",
    "# Be even more patient, as the training time is almost doubled :P\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "n_hidden = 128\n",
    "birnn = BiRNN(n_letters, n_hidden, n_categories)\n",
    "\n",
    "train(birnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "\n",
    "|&nbsp;|&nbsp;|\n",
    "|--|--|\n",
    "|5000 5% (0m 16s) | 2.9060 Ba / Vietnamese ✗ (Arabic)\n",
    "|10000 10% (0m 33s)| 1.9203 Silva / Spanish ✗ (Portuguese)\n",
    "|15000 15% (0m 51s)| 0.8597 Salucci / Italian ✓\n",
    "|20000 20% (1m 10s)| 1.8664 Chaim / Vietnamese ✗ (Chinese)\n",
    "|25000 25% (1m 29s)| 2.5580 Cruz / Spanish ✗ (Portuguese)\n",
    "|30000 30% (1m 50s)| 3.2462 Neusser / Dutch ✗ (Czech)\n",
    "|35000 35% (2m 8s) |0.4040 Ribeiro / Portuguese ✓\n",
    "|40000 40% (2m 27s)| 0.3474 Daher / Arabic ✓\n",
    "|45000 45% (2m 45s)| 1.1283 Guang / Vietnamese ✗ (Chinese)\n",
    "|50000 50% (3m 3s) |1.4291 Mackenzie / Russian ✗ (Scottish)\n",
    "|55000 55% (3m 20s)| 3.2796 Moon / Scottish ✗ (Korean)\n",
    "|60000 60% (3m 38s)| 2.5028 Boutros / Portuguese ✗ (Arabic)\n",
    "|65000 65% (3m 55s)| 0.0673 Schuttmann / German ✓\n",
    "|70000 70% (4m 13s)| 1.3614 Spada / Italian ✓\n",
    "|75000 75% (4m 30s)| 0.4321 Coilean / Irish ✓\n",
    "|80000 80% (4m 47s)| 1.7251 Schneider / Dutch ✗ (German)\n",
    "|85000 85% (5m 4s) |0.3610 Lopez / Spanish ✓\n",
    "|90000 90% (5m 22s)| 1.3607 Tremblay / French ✓\n",
    "|95000 95% (5m 40s)| 1.6964 Knopf / Czech ✗ (German)\n",
    "\n",
    "Simply from the above output, it seems that BiRNN does not bring significant improvement.\n",
    "Though we need more systematic evaluation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "Now you have built and trained models that can tell the nationality of a name. Try with the following functions on your own names!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, x_tensor):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(x_tensor)\n",
    "    return output\n",
    "\n",
    "\n",
    "def predict(model, input_line, n_predictions=3):\n",
    "    print('\\n> %s' % input_line)\n",
    "    with torch.no_grad():\n",
    "        output = evaluate(model, lineToTensor(input_line))\n",
    "\n",
    "        # Get top N categories\n",
    "        topv, topi = output.topk(n_predictions, 1, True)\n",
    "        predictions = []\n",
    "\n",
    "        for i in range(n_predictions):\n",
    "            value = topv[0][i].item()\n",
    "            category_index = topi[0][i].item()\n",
    "            print('(%.2f) %s' % (value, all_categories[category_index]))\n",
    "            predictions.append([value, all_categories[category_index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(birnn, 'Jose')\n",
    "predict(rnn, 'Jose')\n",
    "\n",
    "predict(birnn, 'Robinhood')\n",
    "predict(rnn, 'Robinhood')\n",
    "\n",
    "predict(birnn, 'Zhang')\n",
    "predict(rnn, 'Zhang')\n",
    "\n",
    "predict(birnn, 'Park')\n",
    "predict(rnn, 'Park')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
